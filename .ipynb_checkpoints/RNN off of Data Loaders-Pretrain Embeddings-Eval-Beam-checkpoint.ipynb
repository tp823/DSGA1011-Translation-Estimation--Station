{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an RNN for Machine Translation\n",
    "## Initial Data Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file we will read in the data for the Vietnamese and Chinese to Engish corpuses, build a token2id and char2id mapping, vocabularies and data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an RNN for Machine Translation\n",
    "Initial Data Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "import pickle as pkl\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import io\n",
    "from collections import Counter\n",
    "import sacrebleu\n",
    "\n",
    "random.seed(123)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Global Variables\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "SOS_IDX = 2\n",
    "EOS_IDX = 3\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in pre-trained fasttext embeddings for the three languages\n",
    "### Building loaded embeddings, token2id, id2token and ordered words for all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words embedded is 100,000\n",
      "Total number of words embedded is 100,000\n",
      "Total number of words embedded is 100,000\n"
     ]
    }
   ],
   "source": [
    "# Load Pre-trained Word Vectors\n",
    "def load_embeddings(word2vec, word2id, embedding_dim):\n",
    "    embeddings = np.zeros((len(word2id), embedding_dim))\n",
    "    for word, index in word2id.items():\n",
    "        try:\n",
    "            embeddings[index] = word2vec[word]\n",
    "\n",
    "        except KeyError:\n",
    "            embeddings[index] = np.random.normal(scale=0.6, size=(300,))\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_vectors(fname, num_vecs=None):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "\n",
    "        if num_vecs is None:\n",
    "            pass\n",
    "        else:\n",
    "            if len(data) + 1 > num_vecs:\n",
    "                break\n",
    "    return data\n",
    "\n",
    "\n",
    "def data_dictionary(tokens, vocab_size_limit):\n",
    "    token_counter = Counter()\n",
    "    for token in tokens:\n",
    "        token_counter[token] += 1\n",
    "\n",
    "    vocab, count = zip(*token_counter.most_common(vocab_size_limit))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(4, 4 + len(vocab))))\n",
    "    id2token = ['<pad>', '<unk>','<sos>','<eos>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX\n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    token2id['<sos>'] = SOS_IDX\n",
    "    token2id['<eos>'] = EOS_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "#Load Word Embeddings\n",
    "\n",
    "path= '../pretrained_embeddings/'\n",
    "\n",
    "en_loaded_embeddings = load_vectors(path +'wiki.en.vec',100000)\n",
    "print(\"Total number of words embedded is {:,d}\".format(len(en_loaded_embeddings)))\n",
    "\n",
    "vi_loaded_embeddings = load_vectors(path+'wiki.vi.vec',100000)\n",
    "print(\"Total number of words embedded is {:,d}\".format(len(vi_loaded_embeddings)))\n",
    "\n",
    "zh_loaded_embeddings = load_vectors(path+'wiki.zh.vec',100000)\n",
    "print(\"Total number of words embedded is {:,d}\".format(len(zh_loaded_embeddings)))\n",
    "\n",
    "\n",
    "#Create token2Id, token2Id\n",
    "en_token2id, en_id2token = data_dictionary(list(en_loaded_embeddings.keys()), 100000)\n",
    "vi_token2id, vi_id2token = data_dictionary(list(vi_loaded_embeddings.keys()), 100000)\n",
    "zh_token2id, zh_id2token = data_dictionary(list(zh_loaded_embeddings.keys()), 100000)\n",
    "\n",
    "\n",
    "#Create Emedding Matrix\n",
    "en_loaded_embeddings=load_embeddings(en_loaded_embeddings,en_token2id,300)\n",
    "vi_loaded_embeddings=load_embeddings(vi_loaded_embeddings,vi_token2id,300)\n",
    "zh_loaded_embeddings=load_embeddings(zh_loaded_embeddings,zh_token2id,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vi -> En | Training Examples: 133317\n",
      "Vi -> En | Training Examples: 133317 \n",
      "\n",
      "Vi -> En | Validation Examples: 1268\n",
      "Vi -> En | Validation Examples: 1268 \n",
      "\n",
      "Vi -> En | Testing Examples: 1553\n",
      "Vi -> En | Testing Examples: 1553 \n",
      "\n",
      "Zh -> En | Training Examples: 213377\n",
      "Zh -> En | Training Examples: 213377 \n",
      "\n",
      "Zh -> En | Validation Examples: 1261\n",
      "Zh -> En | Validation Examples: 1261 \n",
      "\n",
      "Zh -> En | Testing Examples: 1397\n",
      "Zh -> En | Testing Examples: 1397 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading in the Vietnamese -> En datasets\n",
    "path1= '../project_data/en-vi/'\n",
    "\n",
    "train_vi_en = []\n",
    "with open(path1 +'train.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        train_vi_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "train_vi_vi = []\n",
    "with open(path1 + 'train.tok.vi') as inputfile:\n",
    "    for line in inputfile:\n",
    "        train_vi_vi.append(line.strip().lower().split(' '))\n",
    "\n",
    "val_vi_en = []\n",
    "with open(path1 + 'dev.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        val_vi_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "val_vi_vi = []\n",
    "with open(path1 +'dev.tok.vi') as inputfile:\n",
    "    for line in inputfile:\n",
    "        val_vi_vi.append(line.strip().lower().split(' '))\n",
    "        \n",
    "test_vi_en = []\n",
    "with open(path1 +'test.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        test_vi_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "test_vi_vi = []\n",
    "with open(path1 + 'test.tok.vi') as inputfile:\n",
    "    for line in inputfile:\n",
    "        test_vi_vi.append(line.strip().lower().split(' '))\n",
    "        \n",
    "        \n",
    "        \n",
    "#Loading in the Chinese -> En datasets\n",
    "path2= '../project_data/en-zh/'\n",
    "\n",
    "train_zh_en = []\n",
    "with open(path2 +'train.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        train_zh_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "i=0\n",
    "train_zh_zh = []\n",
    "fin = io.open(path2 +'train.tok.zh', 'r', encoding='utf-8',newline= '\\n', errors='ignore')\n",
    "for line in fin:\n",
    "    if i % 2 == 0 :\n",
    "        train_zh_zh.append(line.rstrip().split(' '))\n",
    "    i+=1 \n",
    "\n",
    "val_zh_en = []\n",
    "with open(path2 + 'dev.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        val_zh_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "i=0\n",
    "val_zh_zh = []\n",
    "fin = io.open(path2 +'dev.tok.zh', 'r', encoding='utf-8',newline= '\\n', errors='ignore')\n",
    "for line in fin:\n",
    "    if i % 2 == 0 :\n",
    "        val_zh_zh.append(line.rstrip().split(' '))\n",
    "    i+=1\n",
    "\n",
    "test_zh_en = []\n",
    "with open(path2 +'test.tok.en') as inputfile:\n",
    "    for line in inputfile:\n",
    "        test_zh_en.append(line.strip().lower().split(' '))\n",
    "\n",
    "i=0\n",
    "test_zh_zh = []\n",
    "fin = io.open(path2 +'test.tok.zh', 'r', encoding='utf-8',newline= '\\n', errors='ignore')\n",
    "for line in fin:\n",
    "    if i % 2 == 0 :\n",
    "        test_zh_zh.append(line.rstrip().split(' '))\n",
    "    i+=1\n",
    "\n",
    "#Sanity Checking\n",
    "print(\"Vi -> En | Training Examples: \"+str(len(train_vi_en)))\n",
    "print(\"Vi -> En | Training Examples: \"+str(len(train_vi_vi)), '\\n')\n",
    "\n",
    "print(\"Vi -> En | Validation Examples: \"+str(len(val_vi_en)))\n",
    "print(\"Vi -> En | Validation Examples: \"+str(len(val_vi_vi)), '\\n')\n",
    "\n",
    "print(\"Vi -> En | Testing Examples: \"+str(len(test_vi_en)))\n",
    "print(\"Vi -> En | Testing Examples: \"+str(len(test_vi_vi)), '\\n')\n",
    "\n",
    "print(\"Zh -> En | Training Examples: \"+str(len(train_zh_en)))\n",
    "print(\"Zh -> En | Training Examples: \"+str(len(train_zh_zh)), '\\n')\n",
    "\n",
    "print(\"Zh -> En | Validation Examples: \"+str(len(val_zh_en)))\n",
    "print(\"Zh -> En | Validation Examples: \"+str(len(val_zh_zh)), '\\n')\n",
    "\n",
    "print(\"Zh -> En | Testing Examples: \"+str(len(test_zh_en)))\n",
    "print(\"Zh -> En | Testing Examples: \"+str(len(test_zh_zh)), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preserve original data for evaluation\n",
    "train_vi_en_orig = train_vi_en\n",
    "train_vi_vi_orig = train_vi_vi\n",
    "val_vi_en_orig = val_vi_en\n",
    "val_vi_vi_orig = val_vi_vi\n",
    "test_vi_en_orig = test_vi_en\n",
    "test_vi_vi_orig = test_vi_vi\n",
    "\n",
    "train_zh_en_orig = train_zh_en\n",
    "train_zh_zh_orig = train_zh_zh\n",
    "val_zh_en_orig = val_zh_en\n",
    "val_zh_zh_orig = val_zh_zh\n",
    "test_zh_en_orig = test_zh_en\n",
    "test_zh_zh_orig = test_zh_zh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VI_EN_MAX_LENGTH = int(np.percentile([len(sentence) for sentence in train_vi_en+train_vi_vi], 90))+1\n",
    "ZH_EN_MAX_LENGTH = int(np.percentile([len(sentence) for sentence in train_zh_en+train_zh_zh], 90))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_tokens(sentence, language, translator):\n",
    "    if language== 'English':\n",
    "        token2id = en_token2id\n",
    "    elif language== 'Vietnamese':\n",
    "        token2id = vi_token2id\n",
    "    elif language== 'Chinese':\n",
    "        token2id = zh_token2id\n",
    "    tokens = [token2id[token] if token in token2id else UNK_IDX for token in sentence]\n",
    "    if translator == 'vi':\n",
    "        max_len = VI_EN_MAX_LENGTH-1\n",
    "    elif translator == 'zh':\n",
    "        max_len = ZH_EN_MAX_LENGTH-1\n",
    "    tokens=tokens[:max_len]\n",
    "    return tokens\n",
    "\n",
    "def encoding_dataset(dataset, language, translator):\n",
    "    data = [encoding_tokens(tokens, language, translator) for tokens in dataset] \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vi_en = encoding_dataset(train_vi_en, 'English', 'vi')\n",
    "train_vi_vi = encoding_dataset(train_vi_vi, 'Vietnamese', 'vi')\n",
    "test_vi_en = encoding_dataset(test_vi_en, 'English', 'vi')\n",
    "test_vi_vi = encoding_dataset(test_vi_vi, 'Vietnamese', 'vi')\n",
    "val_vi_en = encoding_dataset(val_vi_en, 'English', 'vi')\n",
    "val_vi_vi = encoding_dataset(val_vi_vi, 'Vietnamese', 'vi')\n",
    "\n",
    "train_zh_en = encoding_dataset(train_zh_en, 'English', 'zh')\n",
    "train_zh_zh = encoding_dataset(train_zh_zh, 'Chinese', 'zh')\n",
    "test_zh_en = encoding_dataset(test_zh_en, 'English', 'zh')\n",
    "test_zh_zh = encoding_dataset(test_zh_zh, 'Chinese', 'zh')\n",
    "val_zh_en = encoding_dataset(val_zh_en, 'English', 'zh')\n",
    "val_zh_zh = encoding_dataset(val_zh_zh, 'Chinese', 'zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class translationDataset(Dataset):\n",
    "    def __init__(self, data_list, target_list):\n",
    "        self.data_list=data_list\n",
    "        self.target_list=target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        data = self.data_list[key][:MAX_SAMPLE_LENGTH]\n",
    "        label = self.target_list[key][:MAX_SAMPLE_LENGTH]\n",
    "        return [data, len(data), label, len(label)]\n",
    "\n",
    "def translation_collate_func(batch):\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    for datum in batch:\n",
    "        padded_data = np.pad(np.array(datum[0]+[EOS_IDX]), \n",
    "                                pad_width=((0,MAX_SAMPLE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_data)\n",
    "        padded_label = np.pad(np.array(datum[2]+[EOS_IDX]), \n",
    "                                pad_width=((0,MAX_SAMPLE_LENGTH-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        label_list.append(padded_label)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.from_numpy(np.array(label_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VI -> EN | dataloaders\n",
    "MAX_SAMPLE_LENGTH = VI_EN_MAX_LENGTH\n",
    "\n",
    "vi_en_train_dataset = translationDataset(train_vi_vi, train_vi_en)\n",
    "vi_en_train_loader = torch.utils.data.DataLoader(dataset=vi_en_train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "vi_en_val_dataset = translationDataset(val_vi_vi, val_vi_en)\n",
    "vi_en_val_loader = torch.utils.data.DataLoader(dataset=vi_en_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "vi_en_test_dataset = translationDataset(test_vi_vi, test_vi_en)\n",
    "vi_en_test_loader = torch.utils.data.DataLoader(dataset=vi_en_test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZH -> EN | dataloaders\n",
    "MAX_SAMPLE_LENGTH = ZH_EN_MAX_LENGTH\n",
    "\n",
    "zh_en_train_dataset = translationDataset(train_zh_zh, train_zh_en)\n",
    "zh_en_train_loader = torch.utils.data.DataLoader(dataset=zh_en_train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "zh_en_val_dataset = translationDataset(val_zh_zh, val_zh_en)\n",
    "zh_en_val_loader = torch.utils.data.DataLoader(dataset=zh_en_val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "zh_en_test_dataset = translationDataset(test_zh_zh, test_zh_en)\n",
    "zh_en_test_loader = torch.utils.data.DataLoader(dataset=zh_en_test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=translation_collate_func,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points, string):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(points)\n",
    "    plt.title(string)\n",
    "    plt.savefig((string+'.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, language, drop_rate=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.language = language\n",
    "        if language == 'Vietnamese':\n",
    "             self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(vi_loaded_embeddings), freeze=True)\n",
    "        elif language == 'Chinese':\n",
    "            self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(vi_loaded_embeddings), freeze=True)\n",
    "        self.gru = nn.GRU(300, hidden_size)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        output = self.dropout(embedded)\n",
    "        output, hidden = self.gru(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, drop_rate=0):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(en_loaded_embeddings), freeze=True)\n",
    "        self.gru = nn.GRU(hidden_size + 300, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, len(en_token2id))\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(p=drop_rate)\n",
    "\n",
    "    def forward(self, input, hidden, enc_output):\n",
    "        input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input)).unsqueeze(0)\n",
    "        embedded_concat = torch.cat((embedded, enc_output), dim=2)\n",
    "        output, hidden = self.gru(embedded_concat, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_tensor = input_tensor.transpose(0,1)\n",
    "    target_tensor = target_tensor.transpose(0,1)\n",
    "    \n",
    "    max_length = input_tensor.size(0)\n",
    "    batch_size = input_tensor.size(1)\n",
    "    vocab_size = len(en_token2id)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "    \n",
    "    encoder_output, encoder_hidden = encoder(input_tensor)\n",
    "    encoder_outputs = encoder_output[0,0]\n",
    "\n",
    "    decoder_input = input_tensor[0,:]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "               decoder_input, decoder_hidden, encoder_hidden)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "    else:\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "               decoder_input, decoder_hidden, encoder_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input[di].item() == EOS_IDX:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / max_length\n",
    "\n",
    "def trainIters(loader, encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    language = encoder.language\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for i, (data, labels) in enumerate(loader):\n",
    "            input_tensor = data\n",
    "            target_tensor = labels\n",
    "    \n",
    "            loss = train(input_tensor, target_tensor, encoder,\n",
    "                         decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            print_loss_total += loss\n",
    "            plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses, language)\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 14s (- 29m 13s) (1 10%) 288.6109\n",
      "6m 4s (- 24m 17s) (2 20%) 258.0866\n",
      "9m 1s (- 21m 3s) (3 30%) 268.7757\n",
      "11m 36s (- 17m 24s) (4 40%) 203.6007\n",
      "14m 18s (- 14m 18s) (5 50%) 141.5883\n",
      "17m 4s (- 11m 23s) (6 60%) 188.3119\n",
      "20m 5s (- 8m 36s) (7 70%) 248.2610\n",
      "22m 58s (- 5m 44s) (8 80%) 199.1190\n",
      "25m 6s (- 2m 47s) (9 90%) 131.1419\n",
      "27m 52s (- 0m 0s) (10 100%) 189.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[288.6109364827474,\n",
       " 258.0866222805447,\n",
       " 268.7757048288981,\n",
       " 203.60073102315272,\n",
       " 141.58833770751954,\n",
       " 188.31194305419922,\n",
       " 248.26099853515626,\n",
       " 199.11898278130423,\n",
       " 131.14185490078395,\n",
       " 189.9964643690321]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a61237d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a61216ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Training Model\n",
    "teacher_forcing_ratio = 0.5\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "encoder1 = EncoderRNN(hidden_size, 'Vietnamese', drop_rate = 0.1).to(device)\n",
    "decoder1 = DecoderRNN(hidden_size).to(device)\n",
    "trainIters(vi_en_val_loader, encoder1, decoder1, n_iters=10, print_every=1,plot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    if lang == 'Vietnamese':\n",
    "        out = []\n",
    "        for word in sentence:\n",
    "            if word in vi_token2id:\n",
    "                out.append(vi_token2id[word])\n",
    "            else:\n",
    "                out.append(UNK_IDX)\n",
    "    elif lang == 'English':\n",
    "        out = []\n",
    "        for word in sentence:\n",
    "            if word in en_token2id:\n",
    "                out.append(en_token2id[word])\n",
    "            else:\n",
    "                out.append(UNK_IDX)\n",
    "    elif lang == 'Chinese':\n",
    "        out = []\n",
    "        for word in sentence:\n",
    "            if word in zh_token2id:\n",
    "                out.append(zh_token2id[word])\n",
    "            else:\n",
    "                out.append(UNK_IDX)\n",
    "    return out\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_IDX)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_helper(decoder, decoder_output, decoder_hidden, encoder_hidden, beam_size):\n",
    "    idx = []\n",
    "    probs = []\n",
    "    decoder_output, decoder_hidden = decoder(torch.tensor(decoder_output).to(torch.long).to(device),\n",
    "                decoder_hidden, encoder_hidden)\n",
    "\n",
    "    topv, topi = decoder_output.topk(beam_size)\n",
    "\n",
    "    for x in range(beam_size):\n",
    "        idx.append(topi[0][x].item())\n",
    "        probs.append(topv[0][x].item())\n",
    "\n",
    "    return idx, probs, decoder_hidden\n",
    "\n",
    "def evaluate_beam(encoder, decoder, sentence, input_lang, beam_size=3):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        max_length = input_tensor.size(0)\n",
    "        input_tensor = input_tensor.transpose(0,1)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor)\n",
    "        encoder_outputs = encoder_output[0,0]\n",
    "\n",
    "        decoder_input = input_tensor[0,:]\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "\n",
    "        beam_storage = []\n",
    "        for i in range(beam_size):\n",
    "            beam_storage.append([])\n",
    "            \n",
    "        for di in range(max_length):\n",
    "            beams = []\n",
    "            for x in range(beam_size**2):\n",
    "                beams.append([])\n",
    "\n",
    "            if di == 0: #for the first sentence\n",
    "                probs = np.zeros(beam_size)\n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_hidden)\n",
    "                topv, topi = decoder_output.topk(beam_size)\n",
    "                for x in range(beam_size):\n",
    "                    beam_storage[x].append((decoder_hidden, topi[0][x].item(), topv[0][x].item()))\n",
    "\n",
    "            else:\n",
    "                probs_storage = np.zeros(beam_size**2)\n",
    "                for x, y in enumerate(beam_storage):\n",
    "                    #use our beam helper function to pull out indices, probabilities and hidden layer\n",
    "                    idx, probs, decoder_hidden = beam_helper(decoder, y[-1][1], y[-1][0], encoder_hidden, beam_size)\n",
    "                    beam_prob = 1\n",
    "                    for val in y:\n",
    "                        beam_prob = beam_prob * np.exp(val[2])\n",
    "\n",
    "                    for i, j in enumerate(probs):\n",
    "                        probs_storage[i + x * beamsize] = beam_prob * np.exp(j)\n",
    "\n",
    "                    for i in range(beam_size):\n",
    "                        beams[i + x * beam_size] = y.copy()\n",
    "                        beams[i + x * beam_size].append((decoder_hidden, idx[i], probs[i]))\n",
    "\n",
    "            if di > 0:\n",
    "                ind = np.argpartition(probs_vec, -beam_size)[-beam_size:]\n",
    "                for x, y in enumerate(ind):\n",
    "                    beam_storage[x] = beams[y].copy()\n",
    "                ends = []\n",
    "\n",
    "                for i in beams_keep:\n",
    "                    ends.append(i[-1][1])\n",
    "\n",
    "                if len(set(ends)) == 1 and list(set(ends))[0] == 1:\n",
    "                    print(\"end early\")\n",
    "                    for x in beam_storage:\n",
    "                        words_now = []\n",
    "                        for y in range(len(x)):\n",
    "                            words_now.append(\n",
    "                                output_lang.index2word[x[y][1]])\n",
    "                        decoded_words.append(words_now)\n",
    "                    return decoded_words\n",
    "\n",
    "        if not greedy:\n",
    "            for x in beams_keep:\n",
    "                words_now = []\n",
    "                for y in range(len(x)):\n",
    "                    words_now.append(output_lang.index2word[x[y][1]])\n",
    "                decoded_words.append(words_now)\n",
    "\n",
    "        return decoded_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['cui_cng', ',', 'y', 'l', 'mt', 'ni_c', '26', 'tui_ti', 'c', 'dp', 'ni_chuyn', 'cng', '.']\n",
      "= ['finally', ',', 'this', 'is', 'a', '26-year-old', 'nun', 'i', 'spoke', 'to', '.']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-203ea198f977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moutput_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_beam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Vietnamese\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-e8878a380108>\u001b[0m in \u001b[0;36mevaluate_beam\u001b[0;34m(encoder, decoder, sentence, input_lang, beam_size)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                     \u001b[0;31m#use our beam helper function to pull out indices, probabilities and hidden layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                     \u001b[0mbeam_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-e8878a380108>\u001b[0m in \u001b[0;36mbeam_helper\u001b[0;34m(decoder, decoder_output, decoder_hidden, encoder_hidden, beam_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     decoder_output, decoder_hidden = decoder(torch.tensor(decoder_output).to(torch.long).to(device),\n\u001b[0;32m----> 5\u001b[0;31m                 decoder_hidden, encoder_hidden)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-bcaf14514d85>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, enc_output)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0membedded_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "index = randint(0, len(train_vi_en_orig))\n",
    "sentence1 = train_vi_vi_orig[index] \n",
    "sentence2 = train_vi_en_orig[index]            \n",
    "\n",
    "print('>', sentence1)\n",
    "print('=', sentence2)\n",
    "output_words = evaluate_beam(encoder1, decoder1, sentence1, \"Vietnamese\")\n",
    "output_sentence = ' '.join(output_words)\n",
    "print('<', output_sentence)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "def evaluateRandomly(encoder, decoder, language, n=10):\n",
    "    for i in range(n):\n",
    "        if language == 'Vietnamese':\n",
    "            index = randint(0, len(train_vi_en_orig))\n",
    "            sentence1 = train_vi_vi_orig[index] \n",
    "            sentence2 = train_vi_en_orig[index]\n",
    "        elif language == 'Chinese':\n",
    "            index = randint(0, len(train_zh_en_orig))\n",
    "            sentence1 = train_zh_zh_orig[index]\n",
    "            sentence2 = train_zh_en_orig[index]\n",
    "        \n",
    "        print('>', sentence1)\n",
    "        print('=', sentence2)\n",
    "        output_words = evaluate(encoder, decoder, sentence1, language)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ['chong_chng', 'quay', 'l', 'mt', '\"', 'c_tnh', 'hp', 'tri', '\"', 'xy_ra', 'do', 'tng_tc', 'gia', 'nhng', 'ch', 'ch', 'con', 'm', 'quy_lut', 'duy_nht', 'l', 'c_gng', 'duy_tr', 's', 'tip_cn', 'ca', 'chng', 'vi', 't', 'sa', '.', 'v', 'do', '', ',', 'y', 'chng', 'i', 'theo', 'mt', 'hng', 'ngu_nhin', '.']\n",
      "= ['the', 'pinwheel', 'is', 'an', 'emergent', 'property', 'of', 'the', 'interactions', 'between', 'puppies', 'whose', 'only', 'rule', 'is', 'to', 'try', 'to', 'keep', 'access', 'to', 'the', 'milk', 'and', 'therefore', 'to', 'push', 'in', 'a', 'random', 'direction', '.']\n",
      "< <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "> ['sally', 'tip_li', ':', '\"', 'cho', 's_kin', 'ma_h', '', 'trng', 'nyu.', '\"']\n",
      "= ['sally', ':', '&quot;', 'for', 'this', 'summer', 'program', 'at', 'nyu', '.', '&quot;']\n",
      "< \n",
      "\n",
      "> ['nh', 'mt', 'cu_ni', ',', 'h', 'bit', 'rng', '\"', 'tnh_cch', 'thc_s', 'ca', 'bn', 'ang', 'ln_khut', 'trong', 'bng_ti', '.', '\"']\n",
      "= ['they', 'know', ',', 'as', 'someone', 'once', 'said', ',', '&quot;', 'character', '&apos;s', 'who', 'you', 'are', 'in', 'the', 'dark', '.', '&quot;']\n",
      "< <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "> ['sau', 'tri', 'nim', 'ny', ',', 'thy_gio', 'ca', 'ti', ',', 'thy', 'shilale', ',', '', 'mang', 'nhng', 'quyn', 'chuyn', 'hnh', 'ny', ',', 'v', 'ti', 'ngh', ':', '\"', 'chuyn', 'hnh', 'cho', 'tr_em', '!', '\"']\n",
      "= ['so', 'after', 'this', 'experience', ',', 'my', 'art', 'teacher', ',', 'mr.', 'shilale', ',', 'he', 'brought', 'in', 'these', 'picture', 'books', ',', 'and', 'i', 'thought', ',', '&quot;', 'picture', 'books', 'for', 'kids', '!', '&quot;']\n",
      "< \n",
      "\n",
      "> ['con', 'mun', 'ngi', 'lan', 'rng', 's', 'ni_ting', 'ca', 'con', 'qua', 'mi', 'vng', 'min', '.']\n",
      "= ['i', 'want', 'you', 'to', 'spread', 'the', 'fame', 'of', 'my', 'name', 'through', 'every', 'land', '.']\n",
      "< \n",
      "\n",
      "> ['v', 'th_k', 'th', '13', ',', 'c', 'nhiu', 'hng', 'hn', 'v', 'nhng', 'hnh_dng', 'mi', 'ca', 'nt_nhc', 'c', 'kho', 'li', 'trong', 'khi_nim', 'ca', 'giai_iu', 'mt_cch', 'chnh_xc', ',', 'v', 'iu', 'ny', 'dn', 'n', 'vic', 'hnh_thnh', 'nn', 'nhng', 'nt_nhc', 'm', 'chng_ta', 'c', 'ngy_nay', '.']\n",
      "= ['and', 'then', 'in', 'the', '13th', 'century', ',', 'more', 'lines', 'and', 'new', 'shapes', 'of', 'notes', 'locked', 'in', 'the', 'concept', 'of', 'the', 'tune', 'exactly', ',', 'and', 'that', 'led', 'to', 'the', 'kind', 'of', 'notation', 'we', 'have', 'today', '.']\n",
      "< <pad> <pad> <pad> <pad> <pad> <pad> <pad> cahokia <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "> ['n', 'l', 'mt', 'loi', 'sinh_vt', 'rt', 'c_c', '.']\n",
      "= ['it', '&apos;s', 'a', 'very', 'solitary', 'creature', '.']\n",
      "< <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "> ['mt', 'bn', ',', 'chng_ta', 'c', '1,2', 'kg', ',', 'v', 'mt', 'kia', 'l', '0,6', 'kg', '.']\n",
      "= ['at', 'the', 'one', 'hand', ',', 'we', 'have', '1.2', 'kilos', ',', 'and', 'at', 'the', 'other', 'hand', '0.6', 'kilos', '.']\n",
      "< <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "> ['vi', 'mt', 'tm_trng', 'khng', 'vui', '.', 'hy', 'ngh', 'v', 'iu', 'ny', '.']\n",
      "= ['she', '&apos;s', 'not', 'amused', '.', 'let', '&apos;s', 'think', 'about', 'it', '.']\n",
      "< <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "> ['nhng', 'may_thay', ',', 'c', 'mt', 's_t', ',', 'mt_s', 'ng', 't_ho', '-', 'nh', 'ch', 'y', 'khuyn', 'ti', 'nn', 'c', 'bi', 'ni', 'nh', 'vy', '.', 'v', 'ti', 'hi', 'bn_thn', ',', 'cu_hi', 'm', 'mark', 'zuckerberg', 'ngi', 'sng_lp', 'ra', 'facebook', 'v', 'cng', 'l', 'sp', 'ca', 'ti', ',', '', 'hi', 'tt_c', 'chng_ti', ',', 'rng', ',', 'ti', 's', 'lm', 'g', 'nu_khng', 'cm_thy', 's_hi', '.']\n",
      "= ['but', 'fortunately', ',', 'there', 'were', 'the', 'few', ',', 'the', 'proud', '--', 'like', 'you', '--', 'who', 'told', 'me', 'i', 'should', 'give', 'the', 'speech', ',', 'and', 'i', 'asked', 'myself', 'the', 'question', 'mark', 'zuckerberg', 'might', '--', 'the', 'founder', 'of', 'facebook', 'and', 'my', 'boss', '--', 'asks', 'all', 'of', 'us', ',', 'which', 'is', ',', 'what', 'would', 'i', 'do', 'if', 'i', 'wasn', '&apos;t', 'afraid', '?']\n",
      "< <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = VI_EN_MAX_LENGTH\n",
    "evaluateRandomly(encoder1, decoder1, \"Vietnamese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate BLEU score on corpus level\n",
    "def BLEU(encoder, decoder, language):\n",
    "    hypotheses = \"\"\n",
    "    targets = \"\"\n",
    "    if language == 'Vietnamese':\n",
    "        inputs = val_vi_vi_orig\n",
    "        labels = val_vi_en_orig\n",
    "    else:\n",
    "        inputs = val_zh_zh_orig\n",
    "        labels = val_zh_zh_orig\n",
    "        \n",
    "    for sentence in inputs:\n",
    "        output = evaluate(encoder, decoder, sentence, language)\n",
    "        for word in output:\n",
    "            hypotheses = hypotheses + \" \" + word\n",
    "\n",
    "    #targets\n",
    "    for sentence in labels:\n",
    "        for word in sentence:\n",
    "            if word in en_id2token:\n",
    "                targets = targets + \" \" + word\n",
    "            else:\n",
    "                targets = targets + \" \" + '<unk>'\n",
    "\n",
    "    score = sacrebleu.corpus_bleu(hypotheses, targets)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU(score=0.020322293857056525, counts=[2273, 50, 1, 0], totals=[75975, 75974, 75973, 75972], precisions=[2.9917736097400462, 0.06581198831179087, 0.0013162570913350796, 0.0006581372084452166], bp=1.0, sys_len=75975, ref_len=29000)\n"
     ]
    }
   ],
   "source": [
    "BLEU(encoder1, decoder1, 'Vietnamese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
